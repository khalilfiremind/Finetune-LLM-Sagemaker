{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83954ed4",
   "metadata": {},
   "source": [
    "# Legal Q&A Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c808f47",
   "metadata": {},
   "source": [
    "This notebook generates synthetic legal question-answer pairs using AWS Bedrock's Claude 3.5 Sonnet model. The generated data is designed for fine-tuning legal assistant AI models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7dc68",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f05ce",
   "metadata": {},
   "source": [
    "The notebook takes a set of legal texts and uses Claude to generate high-quality Q&A pairs that are technically accurate and legally grounded. The generated pairs are saved in JSONL format and uploaded to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fdd332",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f3a2a6",
   "metadata": {},
   "source": [
    "First, we import the necessary Python packages for AWS interaction, JSON handling, and progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "def10cf4-7933-4a0f-a8b9-8807ed30122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1266c16c",
   "metadata": {},
   "source": [
    "## 2. AWS Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c0b4b",
   "metadata": {},
   "source": [
    "Set up AWS credentials and initialize clients for Bedrock and S3 services. Make sure you have the appropriate AWS profile configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4fcd488-d601-4202-bc66-f9e708fb6554",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"khalil-adib-bucket\"\n",
    "PROFILE_NAME = \"dev\"\n",
    "REGION_NAME = \"us-east-1\"\n",
    "\n",
    "session = boto3.Session(profile_name=PROFILE_NAME, region_name=REGION_NAME)\n",
    "bedrock_runtime = session.client('bedrock-runtime')\n",
    "s3 = session.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e9260c",
   "metadata": {},
   "source": [
    "## 3. Model and Prompt Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c0cd1",
   "metadata": {},
   "source": [
    "Define the Bedrock model ID and create a prompt template for generating Q&A pairs. The template instructs Claude to focus on technical legal concepts and statutory interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4af973",
   "metadata": {},
   "source": [
    "The `LEGAL_TEXTS` list contains 10 curated legal texts covering various legal concepts and doctrines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49d44a79-2bd9-4a7c-bcad-35374a581f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "NUMBER_OF_PAIRS = 10\n",
    "\n",
    "def get_prompt(text):\n",
    "    return f\"\"\"\n",
    "You are an expert in legal drafting and education. Your task is to generate high-quality question-answer pairs for fine-tuning a legal assistant AI.\n",
    "\n",
    "Instructions:\n",
    "- Focus strictly on technical legal concepts, statutory interpretation, and precedent-based reasoning.\n",
    "- Do NOT include speculative, emotional, or overly general questions.\n",
    "- Base your questions and answers solely on the text provided.\n",
    "- Answers must be precise, well-reasoned, and legally grounded.\n",
    "\n",
    "Input text: {text}\n",
    "\n",
    "Generate exactly {NUMBER_OF_PAIRS} question-answer pairs.\n",
    "\n",
    "Return the result as a JSON array with the following format:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"question\": \"What is the legal implication of ...?\",\n",
    "    \"answer\": \"Under [doctrine/statute], the legal interpretation is ...\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "please only return the array without any text\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "LEGAL_TEXTS = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"text\": \"The plain meaning rule directs that if the language of a statute is clear and unambiguous, the courts must apply it as written, without resorting to extrinsic aids of interpretation. However, where ambiguity exists, legislative history, purpose, and canons of construction may be invoked.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"text\": \"Under the doctrine of stare decisis, courts are generally bound to follow precedents established by higher courts within the same jurisdiction. Exceptions are made when prior rulings are shown to be clearly erroneous or outdated due to legal or social developments.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"text\": \"In Chevron U.S.A. Inc. v. Natural Resources Defense Council, the Supreme Court held that when a statute is ambiguous, courts must defer to an agency’s reasonable interpretation of that statute, provided Congress has not spoken directly to the issue.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"text\": \"Mens rea, or the 'guilty mind', is a fundamental component of criminal liability. Most serious crimes require both a prohibited act (actus reus) and a culpable mental state, which can range from negligence to purposefulness.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"text\": \"The doctrine of res judicata bars parties from re-litigating a claim that has already been finally adjudicated on the merits by a competent court. It promotes judicial efficiency and finality in litigation.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"text\": \"For a contract to be enforceable, it must be supported by consideration, which is defined as a bargained-for exchange of something of legal value. Past consideration or moral obligations generally do not satisfy this requirement.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 7,\n",
    "        \"text\": \"The Commerce Clause grants Congress the authority to regulate commerce among the states. Judicial interpretations have evolved, permitting regulation of intrastate activities that substantially affect interstate commerce.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 8,\n",
    "        \"text\": \"When a law implicates a fundamental right or targets a suspect classification, courts apply strict scrutiny, requiring the law to be narrowly tailored to serve a compelling governmental interest.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 9,\n",
    "        \"text\": \"The Rule Against Perpetuities prevents future interests in property from vesting too remotely. At common law, a contingent interest must vest, if at all, no later than 21 years after the death of a relevant life in being at the time of the creation of the interest.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 10,\n",
    "        \"text\": \"In Miranda v. Arizona, the Supreme Court held that statements made during custodial interrogation are inadmissible unless the suspect was informed of their rights to remain silent and to legal counsel, and voluntarily waived those rights.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b2a98",
   "metadata": {},
   "source": [
    "## 4. Generate Q&A Pairs\n",
    "For each legal text, we:\n",
    "1. Create a prompt using the template\n",
    "2. Call the Bedrock API with random temperature and top-p values for diversity\n",
    "3. Store the generated Q&A pairs\n",
    "\n",
    "The progress bar shows the generation status for all 10 legal texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04639cc7-5405-465d-83b0-8d14e6b78503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 10/10 [03:19<00:00, 19.98s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "for item in tqdm(LEGAL_TEXTS):\n",
    "    try:\n",
    "        prompt = get_prompt(text=item['text'])\n",
    "        response = bedrock_runtime.converse(\n",
    "            modelId=MODEL_ID,\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': [\n",
    "                        {\n",
    "                            'text': prompt,\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            inferenceConfig={\n",
    "                'maxTokens': 8192,\n",
    "                'temperature': random.random(),\n",
    "                'topP': random.random(),\n",
    "            }\n",
    "        )\n",
    "        output = response['output']['message']['content'][0]['text']\n",
    "        dataset.append(output)\n",
    "    except:\n",
    "        print(f'Error at {item}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f135676",
   "metadata": {},
   "source": [
    "## 5. Process Generated Data\n",
    "Convert the raw JSON strings into Python objects and flatten the nested structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00e5811c-0576-4367-af64-42c1afb0b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [ json.loads(d) for d in dataset ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e19b7d97-2fe1-4ccf-8b89-d5883a533660",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = [item for sublist in dataset for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79e550",
   "metadata": {},
   "source": [
    "## 6. Format Dataset\n",
    "Transform the Q&A pairs into the required format with 'prompt' and 'completion' fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b29733b4-a6d4-4105-aaa4-3bd8e9bb4909",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = []\n",
    "for i in flattened:\n",
    "    processed_dataset.append({\n",
    "        'prompt': i['question'],\n",
    "        'completion': i['answer'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3da60c",
   "metadata": {},
   "source": [
    "## 7. Save Dataset\n",
    "Save the processed dataset to a local JSONL file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6efad3f5-ab6e-4e82-b9a1-253bf2e172c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset.jsonl\", \"w\", ) as f:\n",
    "    for item in processed_dataset:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf13019-b303-4f29-b815-361ea6670415",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file('dataset.jsonl', BUCKET_NAME, 'webinar/dataset/webinar_dataset.jsonl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

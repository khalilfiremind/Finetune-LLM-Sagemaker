{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0291868",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama 3 8B Instruct Model with SageMaker\n",
    "\n",
    "This notebook demonstrates how to fine-tune the Llama 3 8B Instruct model using Amazon SageMaker. We'll use the JumpStart feature to simplify the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a51c04f-3415-48ee-ba32-a19b40717877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "from uuid import uuid4\n",
    "import sagemaker\n",
    "from sagemaker import hyperparameters\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975f2dde",
   "metadata": {},
   "source": [
    "## Setup AWS Configuration\n",
    "\n",
    "First, we'll set up our AWS configuration including the profile, region, and S3 bucket details. This establishes our connection to AWS services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afad2a9a-9e47-4335-b1e9-3252b04f4088",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROFILE_NAME = \"dev\"\n",
    "REGION_NAME = \"us-east-1\"\n",
    "BUCKET_NAME = \"khalil-adib-bucket\"\n",
    "\n",
    "session = boto3.Session(profile_name=PROFILE_NAME, region_name=REGION_NAME)\n",
    "s3_client = session.client('s3')\n",
    "sagemaker_session = sagemaker.Session(boto_session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d4fec",
   "metadata": {},
   "source": [
    "## Define Training Template\n",
    "\n",
    "We'll create a template for instruction tuning that defines how the model should process inputs and generate outputs. This template will be used to format our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f684fa91-c874-4223-9f06-61c66a993d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_train_template = {\n",
    "    \"prompt\": \"Answer user query, be helpful, don't come up with facts. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Input:\\n{prompt}\\n\\n\",\n",
    "    \"completion\": \" {completion}\",\n",
    "}\n",
    "\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(instruct_train_template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d67b78",
   "metadata": {},
   "source": [
    "## Upload Template to S3\n",
    "\n",
    "The training template needs to be uploaded to S3 so it can be accessed by the SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa06fc0f-75cc-4380-96a0-d73d44c9a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"webinar\"\n",
    "input_root = f\"{data_root}/dataset/\"\n",
    "output_key = f\"{data_root}/output/\"\n",
    "\n",
    "template_params = {\n",
    "    \"Filename\": \"template.json\",\n",
    "    \"Bucket\": BUCKET_NAME,\n",
    "    \"Key\": f\"{data_root}template.json\"\n",
    "}\n",
    "\n",
    "s3_client.upload_file(**template_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adb4f75",
   "metadata": {},
   "source": [
    "## Configure Training Parameters\n",
    "\n",
    "Here we set up the essential parameters for our fine-tuning job:\n",
    "- Training data location\n",
    "- Output path for the model\n",
    "- Instance type (using ml.g5.12xlarge for optimal performance)\n",
    "- IAM role for SageMaker execution\n",
    "- Model ID and version (Llama 3 8B Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f13358a-9936-4e1c-97c0-7732432b0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_uri = f\"s3://{BUCKET_NAME}/{data_root}\"\n",
    "output_path = f\"s3://{BUCKET_NAME}/{output_key}\"\n",
    "\n",
    "JOB_NAME = f\"webinar-finetine-job-{str(uuid4())}\"\n",
    "\n",
    "INSTANCE_TYPE = \"ml.g5.12xlarge\"\n",
    "ROLE = \"arn:aws:iam::026090512591:role/sagemaker-execution-role-SageMakerExecutionRole-lZm8CUm9jqkj\"\n",
    "\n",
    "model_id = \"meta-textgeneration-llama-3-8b-instruct\"\n",
    "model_version = \"2.13.0\"\n",
    "accept_eula = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d603ecdf",
   "metadata": {},
   "source": [
    "## Customize Hyperparameters\n",
    "\n",
    "We'll modify the default hyperparameters to suit our specific use case:\n",
    "- Enable instruction tuning\n",
    "- Disable chat dataset format\n",
    "- Set maximum input length\n",
    "- Configure validation split ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52c68b5f-bff6-4e6a-b3ff-2a6ff08ee497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'int8_quantization': 'False',\n",
       " 'enable_fsdp': 'True',\n",
       " 'epoch': '1',\n",
       " 'learning_rate': '0.0001',\n",
       " 'lora_r': '8',\n",
       " 'lora_alpha': '32',\n",
       " 'target_modules': 'q_proj,v_proj',\n",
       " 'lora_dropout': '0.05',\n",
       " 'instruction_tuned': 'False',\n",
       " 'chat_dataset': 'True',\n",
       " 'add_input_output_demarcation_key': 'True',\n",
       " 'per_device_train_batch_size': '1',\n",
       " 'per_device_eval_batch_size': '1',\n",
       " 'max_train_samples': '-1',\n",
       " 'max_val_samples': '-1',\n",
       " 'seed': '10',\n",
       " 'max_input_length': '-1',\n",
       " 'validation_split_ratio': '0.2',\n",
       " 'train_data_split_seed': '0',\n",
       " 'preprocessing_num_workers': 'None'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webinar_training_hyperparamter = hyperparameters.retrieve_default(\n",
    "    model_id=model_id, model_version=model_version, sagemaker_session=sagemaker_session\n",
    ")\n",
    "webinar_training_hyperparamter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26334011-5dc0-4c19-bdeb-e3b479a569d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "webinar_training_hyperparamter['instruction_tuned'] = 'True'\n",
    "webinar_training_hyperparamter['chat_dataset'] = 'False'\n",
    "webinar_training_hyperparamter['max_input_length'] = '512'\n",
    "webinar_training_hyperparamter['validation_split_ratio'] = '0.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2bba4f89-e9ea-4d6a-a2b7-8dd3f5b652a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters.validate(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    hyperparameters=webinar_training_hyperparamter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cae8bf6",
   "metadata": {},
   "source": [
    "## Initialize and Start Training\n",
    "\n",
    "Now we'll create the JumpStart estimator and start the fine-tuning job. This will:\n",
    "1. Initialize the training environment\n",
    "2. Configure the model parameters\n",
    "3. Start the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9bc5af-2795-4d8d-ab8c-e6304e5f0e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    role=ROLE,\n",
    "    environment={\"accept_eula\": \"true\"},  # set \"accept_eula\": \"true\" to accept the EULA for gated models\n",
    "    disable_output_compression=True,\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    hyperparameters=webinar_training_hyperparamter,\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "# Start the fine-tuning job\n",
    "estimator.fit(inputs={\"training\": train_data_uri}, job_name=JOB_NAME, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9cbad",
   "metadata": {},
   "source": [
    "## Monitor Training Progress\n",
    "\n",
    "This cell will monitor the training job status and provide updates until completion. The training process may take several hours depending on your dataset size and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb32d6c-5900-4746-a92c-7be66d6a242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_status = sagemaker_session.describe_training_job(job_name=JOB_NAME)\n",
    "\n",
    "continue_check = True\n",
    "\n",
    "while continue_check:\n",
    "    job_status = sagemaker_session.describe_training_job(job_name=JOB_NAME)\n",
    "    print(job_status['TrainingJobStatus'])\n",
    "    if job_status['TrainingJobStatus'] != 'InProgress':\n",
    "        print(job_status['TrainingJobStatus'])\n",
    "        continue_check = False\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e2e08f-5363-40c0-b48a-e41c99372546",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_status['ModelArtifacts']['S3ModelArtifacts']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65b418",
   "metadata": {},
   "source": [
    "## Access Model Artifacts\n",
    "\n",
    "Once training is complete, we can access the model artifacts from S3. These artifacts contain the fine-tuned model that can be deployed for inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
